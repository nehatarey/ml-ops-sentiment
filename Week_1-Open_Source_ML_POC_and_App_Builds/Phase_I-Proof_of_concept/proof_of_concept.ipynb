{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align = \"center\" draggable=‚Äùfalse‚Äù ><img src=\"https://user-images.githubusercontent.com/37101144/161836199-fdb0219d-0361-4988-bf26-48b0fad160a3.png\"\n",
    "     width=\"200px\"\n",
    "     height=\"auto\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "# <h1 align=\"center\" id=\"heading\">Phase I - Proof of Concept</h1>\n",
    "\n",
    "\n",
    "\n",
    "## ‚òëÔ∏è Objectives\n",
    "At the end of this session, you will have a brief understanding of how to:\n",
    "- [ ] Find and run pre-trained models (Phase I)\n",
    "- [ ] Evaluate results from pre-trained models (Phase I)\n",
    "- [ ] Run a pre-trained model using real Reddit data (Phase I)\n",
    "\n",
    "\n",
    "## üõ†Ô∏è Pre-Assignment\n",
    "1. Create a virtual environment with üêç conda : `conda env create -f environment.yml`\n",
    "\n",
    "2. Activate your conda virtual environment: `conda activate tsla_bot`\n",
    "\n",
    "3. Create a .env file in the root directory and add the following variables:\n",
    "4. \n",
    "   `STOCK_API_KEY` : API key from [twelvedata](https://twelvedata.com/pricing)\n",
    "\n",
    "   `REDDIT_API_CLIENT_ID` : client ID of your reddit app\n",
    "   \n",
    "   `REDDIT_API_CLIENT_SECRET`: client secret of your reddit app\n",
    "   \n",
    "   Follow this tutorial to generate your own Reddit credentials:\n",
    "   <https://www.jcchouinard.com/get-reddit-api-credentials-with-praw/>\n",
    "\n",
    "4. Continue in this notebook\n",
    "\n",
    "\n",
    "\n",
    "## Background\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the meeting with your boss, and reviewing your [notes](https://www.notion.so/Analyzing-Market-Sentiment-Phase-I-II-and-II-End-to-End-MLOps-with-Open-Source-Tools-dc4b846108b44f6bb2962d550368560c#54cc350bc95041ee873dabde36930af1) üìì, you're ready to get going on a Proof of Concept (POC)\n",
    "\n",
    "A POC tests the validity of your hypothesis. It's a way to prove that your idea, task, app, or whatever else works!\n",
    "\n",
    "There's no time to waste - you've got an idea - it's time to get testing it out! üèÅ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Initial Imports and Variable Setting üìà\n",
    "\n",
    "First things first: Let's set some variables that will help us going forward.\n",
    "\n",
    "Though your boss suggested `\"TSLA\"`, you can use any active stock-symbol and subreddit!\n",
    "\n",
    "**IMPORTANT**: Make sure you verify that your selected subreddit exists by navigating to the generated link after you run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reddit.com/r/TSLA\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "### START CODE HERE\n",
    "\n",
    "# Stock data to grab ex. \"TSLA\"\n",
    "symbol = \"TSLA\"\n",
    "\n",
    "# subreddit to check\n",
    "subreddit = 'TSLA'\n",
    "\n",
    "# Time interval granularity\n",
    "# valid choices are \"1week\", \"1month\", \"1day\"\n",
    "interval = '1day'\n",
    "\n",
    "# set the beginning and end of the time range you'd like to analyze\n",
    "# ensure you use the format \"YYYY-MM-DD\"\n",
    "start_date = \"2022-07-07\"\n",
    "end_date = \"2022-07-08\"\n",
    "\n",
    "### END CODE HERE\n",
    "\n",
    "start_date_dt = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "end_date_dt = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "\n",
    "print(f'reddit.com/r/{subreddit}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make sure we can import our `BotUtils.py`, we have to ensure we're running from the `TSLASentimentAnalyzer` folder. To do this, we can `cd` into that directory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nehatarey/mlops-course/phases/code/ml-ops-sentiment/Week_1-Open_Source_ML_POC_and_App_Builds/Phase_I-Proof_of_concept/TSLASentimentAnalyzer\n"
     ]
    }
   ],
   "source": [
    "#%cd \"TSLASentimentAnalyzer\"\n",
    "#%pwd \n",
    "%cd \"TSLASentimentAnalyzer\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Set Environment Variables ‚õ∞Ô∏è\n",
    "\n",
    "Use the information you created in the instructions of the `README.md` to fill in the values below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/nehatarey/mlops-course/phases/code/ml-ops-sentiment/Week_1-Open_Source_ML_POC_and_App_Builds/Phase_I-Proof_of_concept'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "with open(\".env\", \"r\") as env_file:\n",
    "    #print(env_file.read())\n",
    "    for line in env_file:\n",
    "        key, value = line.strip().split(\":\")\n",
    "        if key and value:\n",
    "            os.environ[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define env variables for configuration\n",
    "import os\n",
    "#import \n",
    "os.environ['REDDIT_API_CLIENT_ID'] = os.environ['REDDIT_API_CLIENT_ID']\n",
    "os.environ['REDDIT_API_CLIENT_SECRET'] = os.environ['REDDIT_API_CLIENT_SECRET']\n",
    "os.environ['STOCK_DATA_API_KEY'] = os.environ['STOCK_API_KEY']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Get to Scraping üåê\n",
    "\n",
    "Now we're going to use the `scraper` module (found in the `TSLASentimentAnalyzer` folder) to \"scrape\" the subreddit of our choosing for posts!\n",
    "\n",
    "‚öóÔ∏è RESOURCES: \n",
    "\n",
    "[Web Scraping](https://www.parsehub.com/blog/what-is-web-scraping/)\n",
    "\n",
    "[Reddit Post Options](https://www.reddit.com/r/help/comments/32eu8w/what_is_the_difference_between_newrising_hot_top/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initializing the Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/nehatarey/mlops-course/phases/code/ml-ops-sentiment/Week_1-Open_Source_ML_POC_and_App_Builds/Phase_I-Proof_of_concept'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nehatarey/mlops-course/phases/code/ml-ops-sentiment/Week_1-Open_Source_ML_POC_and_App_Builds/Phase_I-Proof_of_concept/TSLASentimentAnalyzer\n"
     ]
    }
   ],
   "source": [
    "%cd \"TSLASentimentAnalyzer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<praw.models.listing.generator.ListingGenerator object at 0x7ff1df5a9430>\n",
      "reddit_api_client_id='TPqa-uRZ9eOHlsehEA378A' reddit_api_client_secret='jMgk7xhGBVPjCNWGvQ0CGcvulpLGAA' stock_data_api_key='c5d2186ede5c4d2882377a4ceadc147e' reddit_api_user_agent='USERAGENT' model_path='fourthbrain-demo/model_trained_by_me2'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from loguru import logger\n",
    "from TSLASentimentAnalyzer.classifier import predict\n",
    "from TSLASentimentAnalyzer.scraper import RedditScraper\n",
    "from TSLASentimentAnalyzer.config import settings\n",
    "\n",
    "#settings\n",
    "settings.reddit_api_client_id= os.environ['REDDIT_API_CLIENT_ID']\n",
    "settings.reddit_api_client_secret = os.environ['REDDIT_API_CLIENT_SECRET']\n",
    "settings.stock_data_api_key = os.environ['STOCK_DATA_API_KEY']\n",
    "# instantiating the reddit scraper\n",
    "reddit = RedditScraper('teslainvestorsclub')\n",
    "print(reddit.subreddit.top(limit=10))\n",
    "print(settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Helper Functions üî®\n",
    "\n",
    "Here are some helper functions to assist in collecting data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "\n",
    "def load_comments(number: int, scraping_option: Callable) -> pd.DataFrame:\n",
    "    '''\n",
    "    loads comments from reddit using the RedditScraper using one of the options\n",
    "    and returns a DataFrame\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    number : int\n",
    "        number of posts to load comments from\n",
    "\n",
    "    scraping_options: callable\n",
    "        expects a select_scrap_type function\n",
    "    '''\n",
    "    comments = []\n",
    "    for submission in scraping_option(number):\n",
    "        comments.extend(reddit.get_comment_forest(submission.comments))\n",
    "        print(comments)\n",
    "        logger.debug(\n",
    "            submission.title,\n",
    "            submission.num_comments,\n",
    "            len(reddit.get_comment_forest(submission.comments)),\n",
    "        )\n",
    "    df = pd.DataFrame(comments)\n",
    "    return df\n",
    "\n",
    "\n",
    "def select_scrap_type(option: str) -> Callable:\n",
    "    '''\n",
    "    selects a method from the reddit object based on a given option\n",
    "    '''\n",
    "    if option == \"Hot\":\n",
    "        return reddit.get_hot\n",
    "        \n",
    "    if option == \"Rising\":\n",
    "        return reddit.get_rising\n",
    "\n",
    "    if option == \"New\":\n",
    "        return reddit.get_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading & Processing the Reddit Comment Data\n",
    "\n",
    "Let's scrape the 15 üî• hottest üî• posts from your selected subreddit using the `load_data()` helper function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResponseException",
     "evalue": "received 401 HTTP response",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResponseException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/nehatarey/mlops-course/phases/code/ml-ops-sentiment/Week_1-Open_Source_ML_POC_and_App_Builds/Phase_I-Proof_of_concept/proof_of_concept.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nehatarey/mlops-course/phases/code/ml-ops-sentiment/Week_1-Open_Source_ML_POC_and_App_Builds/Phase_I-Proof_of_concept/proof_of_concept.ipynb#ch0000014?line=0'>1</a>\u001b[0m \u001b[39m# Scraping data from reddit\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nehatarey/mlops-course/phases/code/ml-ops-sentiment/Week_1-Open_Source_ML_POC_and_App_Builds/Phase_I-Proof_of_concept/proof_of_concept.ipynb#ch0000014?line=1'>2</a>\u001b[0m select_scrap_type(\u001b[39m\"\u001b[39m\u001b[39mHot\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nehatarey/mlops-course/phases/code/ml-ops-sentiment/Week_1-Open_Source_ML_POC_and_App_Builds/Phase_I-Proof_of_concept/proof_of_concept.ipynb#ch0000014?line=2'>3</a>\u001b[0m dfReddit \u001b[39m=\u001b[39m load_comments(\u001b[39m1\u001b[39;49m, select_scrap_type(\u001b[39m\"\u001b[39;49m\u001b[39mHot\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n",
      "\u001b[1;32m/Users/nehatarey/mlops-course/phases/code/ml-ops-sentiment/Week_1-Open_Source_ML_POC_and_App_Builds/Phase_I-Proof_of_concept/proof_of_concept.ipynb Cell 19\u001b[0m in \u001b[0;36mload_comments\u001b[0;34m(number, scraping_option)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nehatarey/mlops-course/phases/code/ml-ops-sentiment/Week_1-Open_Source_ML_POC_and_App_Builds/Phase_I-Proof_of_concept/proof_of_concept.ipynb#ch0000014?line=3'>4</a>\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nehatarey/mlops-course/phases/code/ml-ops-sentiment/Week_1-Open_Source_ML_POC_and_App_Builds/Phase_I-Proof_of_concept/proof_of_concept.ipynb#ch0000014?line=4'>5</a>\u001b[0m \u001b[39mloads comments from reddit using the RedditScraper using one of the options\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nehatarey/mlops-course/phases/code/ml-ops-sentiment/Week_1-Open_Source_ML_POC_and_App_Builds/Phase_I-Proof_of_concept/proof_of_concept.ipynb#ch0000014?line=5'>6</a>\u001b[0m \u001b[39mand returns a DataFrame\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nehatarey/mlops-course/phases/code/ml-ops-sentiment/Week_1-Open_Source_ML_POC_and_App_Builds/Phase_I-Proof_of_concept/proof_of_concept.ipynb#ch0000014?line=13'>14</a>\u001b[0m \u001b[39m    expects a select_scrap_type function\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nehatarey/mlops-course/phases/code/ml-ops-sentiment/Week_1-Open_Source_ML_POC_and_App_Builds/Phase_I-Proof_of_concept/proof_of_concept.ipynb#ch0000014?line=14'>15</a>\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nehatarey/mlops-course/phases/code/ml-ops-sentiment/Week_1-Open_Source_ML_POC_and_App_Builds/Phase_I-Proof_of_concept/proof_of_concept.ipynb#ch0000014?line=15'>16</a>\u001b[0m comments \u001b[39m=\u001b[39m []\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/nehatarey/mlops-course/phases/code/ml-ops-sentiment/Week_1-Open_Source_ML_POC_and_App_Builds/Phase_I-Proof_of_concept/proof_of_concept.ipynb#ch0000014?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m submission \u001b[39min\u001b[39;00m scraping_option(number):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nehatarey/mlops-course/phases/code/ml-ops-sentiment/Week_1-Open_Source_ML_POC_and_App_Builds/Phase_I-Proof_of_concept/proof_of_concept.ipynb#ch0000014?line=17'>18</a>\u001b[0m     comments\u001b[39m.\u001b[39mextend(reddit\u001b[39m.\u001b[39mget_comment_forest(submission\u001b[39m.\u001b[39mcomments))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nehatarey/mlops-course/phases/code/ml-ops-sentiment/Week_1-Open_Source_ML_POC_and_App_Builds/Phase_I-Proof_of_concept/proof_of_concept.ipynb#ch0000014?line=18'>19</a>\u001b[0m     \u001b[39mprint\u001b[39m(comments)\n",
      "File \u001b[0;32m~/anaconda3/envs/tsla_bot/lib/python3.8/site-packages/praw/models/listing/generator.py:63\u001b[0m, in \u001b[0;36mListingGenerator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m()\n\u001b[1;32m     62\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_listing \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_list_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_listing):\n\u001b[0;32m---> 63\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_batch()\n\u001b[1;32m     65\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_list_index \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     66\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39myielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/tsla_bot/lib/python3.8/site-packages/praw/models/listing/generator.py:89\u001b[0m, in \u001b[0;36mListingGenerator._next_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exhausted:\n\u001b[1;32m     87\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m()\n\u001b[0;32m---> 89\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_listing \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reddit\u001b[39m.\u001b[39;49mget(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murl, params\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams)\n\u001b[1;32m     90\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_listing \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extract_sublist(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_listing)\n\u001b[1;32m     91\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_list_index \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/tsla_bot/lib/python3.8/site-packages/praw/util/deprecate_args.py:43\u001b[0m, in \u001b[0;36m_deprecate_args.<locals>.wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m     arg_string \u001b[39m=\u001b[39m _generate_arg_string(_old_args[: \u001b[39mlen\u001b[39m(args)])\n\u001b[1;32m     37\u001b[0m     warn(\n\u001b[1;32m     38\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPositional arguments for \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m!r}\u001b[39;00m\u001b[39m will no longer be\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m supported in PRAW 8.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mCall this function with \u001b[39m\u001b[39m{\u001b[39;00marg_string\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     40\u001b[0m         \u001b[39mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m     41\u001b[0m         stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m     42\u001b[0m     )\n\u001b[0;32m---> 43\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mdict\u001b[39;49m(\u001b[39mzip\u001b[39;49m(_old_args, args)), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tsla_bot/lib/python3.8/site-packages/praw/reddit.py:634\u001b[0m, in \u001b[0;36mReddit.get\u001b[0;34m(self, path, params)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[39m@_deprecate_args\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mpath\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    622\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\n\u001b[1;32m    623\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    626\u001b[0m     params: Optional[Union[\u001b[39mstr\u001b[39m, Dict[\u001b[39mstr\u001b[39m, Union[\u001b[39mstr\u001b[39m, \u001b[39mint\u001b[39m]]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    627\u001b[0m ):\n\u001b[1;32m    628\u001b[0m     \u001b[39m\"\"\"Return parsed objects returned from a GET request to ``path``.\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \n\u001b[1;32m    630\u001b[0m \u001b[39m    :param path: The path to fetch.\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[39m    :param params: The query parameters to add to the request (default: ``None``).\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \n\u001b[1;32m    633\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_objectify_request(method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mGET\u001b[39;49m\u001b[39m\"\u001b[39;49m, params\u001b[39m=\u001b[39;49mparams, path\u001b[39m=\u001b[39;49mpath)\n",
      "File \u001b[0;32m~/anaconda3/envs/tsla_bot/lib/python3.8/site-packages/praw/reddit.py:739\u001b[0m, in \u001b[0;36mReddit._objectify_request\u001b[0;34m(self, data, files, json, method, params, path)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_objectify_request\u001b[39m(\n\u001b[1;32m    714\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    715\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    721\u001b[0m     path: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    722\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    723\u001b[0m     \u001b[39m\"\"\"Run a request through the ``Objector``.\u001b[39;00m\n\u001b[1;32m    724\u001b[0m \n\u001b[1;32m    725\u001b[0m \u001b[39m    :param data: Dictionary, bytes, or file-like object to send in the body of the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    736\u001b[0m \n\u001b[1;32m    737\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_objector\u001b[39m.\u001b[39mobjectify(\n\u001b[0;32m--> 739\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    740\u001b[0m             data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    741\u001b[0m             files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    742\u001b[0m             json\u001b[39m=\u001b[39;49mjson,\n\u001b[1;32m    743\u001b[0m             method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    744\u001b[0m             params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    745\u001b[0m             path\u001b[39m=\u001b[39;49mpath,\n\u001b[1;32m    746\u001b[0m         )\n\u001b[1;32m    747\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/tsla_bot/lib/python3.8/site-packages/praw/util/deprecate_args.py:43\u001b[0m, in \u001b[0;36m_deprecate_args.<locals>.wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m     arg_string \u001b[39m=\u001b[39m _generate_arg_string(_old_args[: \u001b[39mlen\u001b[39m(args)])\n\u001b[1;32m     37\u001b[0m     warn(\n\u001b[1;32m     38\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPositional arguments for \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m!r}\u001b[39;00m\u001b[39m will no longer be\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m supported in PRAW 8.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mCall this function with \u001b[39m\u001b[39m{\u001b[39;00marg_string\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     40\u001b[0m         \u001b[39mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m     41\u001b[0m         stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m     42\u001b[0m     )\n\u001b[0;32m---> 43\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mdict\u001b[39;49m(\u001b[39mzip\u001b[39;49m(_old_args, args)), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tsla_bot/lib/python3.8/site-packages/praw/reddit.py:941\u001b[0m, in \u001b[0;36mReddit.request\u001b[0;34m(self, data, files, json, method, params, path)\u001b[0m\n\u001b[1;32m    939\u001b[0m     \u001b[39mraise\u001b[39;00m ClientException(\u001b[39m\"\u001b[39m\u001b[39mAt most one of `data` or `json` is supported.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    940\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 941\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_core\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    942\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    943\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    944\u001b[0m         json\u001b[39m=\u001b[39;49mjson,\n\u001b[1;32m    945\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    946\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    947\u001b[0m         path\u001b[39m=\u001b[39;49mpath,\n\u001b[1;32m    948\u001b[0m     )\n\u001b[1;32m    949\u001b[0m \u001b[39mexcept\u001b[39;00m BadRequest \u001b[39mas\u001b[39;00m exception:\n\u001b[1;32m    950\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/tsla_bot/lib/python3.8/site-packages/prawcore/sessions.py:330\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, path, data, files, json, params, timeout)\u001b[0m\n\u001b[1;32m    328\u001b[0m     json[\u001b[39m\"\u001b[39m\u001b[39mapi_type\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mjson\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    329\u001b[0m url \u001b[39m=\u001b[39m urljoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_requestor\u001b[39m.\u001b[39moauth_url, path)\n\u001b[0;32m--> 330\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request_with_retries(\n\u001b[1;32m    331\u001b[0m     data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    332\u001b[0m     files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    333\u001b[0m     json\u001b[39m=\u001b[39;49mjson,\n\u001b[1;32m    334\u001b[0m     method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    335\u001b[0m     params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    336\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    337\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    338\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/tsla_bot/lib/python3.8/site-packages/prawcore/sessions.py:228\u001b[0m, in \u001b[0;36mSession._request_with_retries\u001b[0;34m(self, data, files, json, method, params, timeout, url, retry_strategy_state)\u001b[0m\n\u001b[1;32m    226\u001b[0m retry_strategy_state\u001b[39m.\u001b[39msleep()\n\u001b[1;32m    227\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_request(data, method, params, url)\n\u001b[0;32m--> 228\u001b[0m response, saved_exception \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    229\u001b[0m     data,\n\u001b[1;32m    230\u001b[0m     files,\n\u001b[1;32m    231\u001b[0m     json,\n\u001b[1;32m    232\u001b[0m     method,\n\u001b[1;32m    233\u001b[0m     params,\n\u001b[1;32m    234\u001b[0m     retry_strategy_state,\n\u001b[1;32m    235\u001b[0m     timeout,\n\u001b[1;32m    236\u001b[0m     url,\n\u001b[1;32m    237\u001b[0m )\n\u001b[1;32m    239\u001b[0m do_retry \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    241\u001b[0m     response \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[39mand\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m codes[\u001b[39m\"\u001b[39m\u001b[39munauthorized\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    243\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/envs/tsla_bot/lib/python3.8/site-packages/prawcore/sessions.py:185\u001b[0m, in \u001b[0;36mSession._make_request\u001b[0;34m(self, data, files, json, method, params, retry_strategy_state, timeout, url)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_make_request\u001b[39m(\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    175\u001b[0m     data,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m     url,\n\u001b[1;32m    183\u001b[0m ):\n\u001b[1;32m    184\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 185\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_rate_limiter\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m    186\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_requestor\u001b[39m.\u001b[39;49mrequest,\n\u001b[1;32m    187\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_header_callback,\n\u001b[1;32m    188\u001b[0m             method,\n\u001b[1;32m    189\u001b[0m             url,\n\u001b[1;32m    190\u001b[0m             allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    191\u001b[0m             data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    192\u001b[0m             files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    193\u001b[0m             json\u001b[39m=\u001b[39;49mjson,\n\u001b[1;32m    194\u001b[0m             params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    195\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    196\u001b[0m         )\n\u001b[1;32m    197\u001b[0m         log\u001b[39m.\u001b[39mdebug(\n\u001b[1;32m    198\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mResponse: \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m (\u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mcontent-length\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m bytes)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    200\u001b[0m         )\n\u001b[1;32m    201\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tsla_bot/lib/python3.8/site-packages/prawcore/rate_limit.py:33\u001b[0m, in \u001b[0;36mRateLimiter.call\u001b[0;34m(self, request_function, set_header_callback, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39m\"\"\"Rate limit the call to request_function.\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m \u001b[39m:param request_function: A function call that returns an HTTP response object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelay()\n\u001b[0;32m---> 33\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m set_header_callback()\n\u001b[1;32m     34\u001b[0m response \u001b[39m=\u001b[39m request_function(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     35\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate(response\u001b[39m.\u001b[39mheaders)\n",
      "File \u001b[0;32m~/anaconda3/envs/tsla_bot/lib/python3.8/site-packages/prawcore/sessions.py:283\u001b[0m, in \u001b[0;36mSession._set_header_callback\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_set_header_callback\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_authorizer\u001b[39m.\u001b[39mis_valid() \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\n\u001b[1;32m    281\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_authorizer, \u001b[39m\"\u001b[39m\u001b[39mrefresh\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     ):\n\u001b[0;32m--> 283\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_authorizer\u001b[39m.\u001b[39;49mrefresh()\n\u001b[1;32m    284\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mAuthorization\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbearer \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_authorizer\u001b[39m.\u001b[39maccess_token\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m}\n",
      "File \u001b[0;32m~/anaconda3/envs/tsla_bot/lib/python3.8/site-packages/prawcore/auth.py:314\u001b[0m, in \u001b[0;36mDeviceIDAuthorizer.refresh\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m     additional_kwargs[\u001b[39m\"\u001b[39m\u001b[39mscope\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_scopes)\n\u001b[1;32m    313\u001b[0m grant_type \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://oauth.reddit.com/grants/installed_client\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 314\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request_token(\n\u001b[1;32m    315\u001b[0m     grant_type\u001b[39m=\u001b[39;49mgrant_type,\n\u001b[1;32m    316\u001b[0m     device_id\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_device_id,\n\u001b[1;32m    317\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49madditional_kwargs,\n\u001b[1;32m    318\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/tsla_bot/lib/python3.8/site-packages/prawcore/auth.py:155\u001b[0m, in \u001b[0;36mBaseAuthorizer._request_token\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    151\u001b[0m url \u001b[39m=\u001b[39m (\n\u001b[1;32m    152\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_authenticator\u001b[39m.\u001b[39m_requestor\u001b[39m.\u001b[39mreddit_url \u001b[39m+\u001b[39m const\u001b[39m.\u001b[39mACCESS_TOKEN_PATH\n\u001b[1;32m    153\u001b[0m )\n\u001b[1;32m    154\u001b[0m pre_request_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 155\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_authenticator\u001b[39m.\u001b[39;49m_post(url, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdata)\n\u001b[1;32m    156\u001b[0m payload \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mjson()\n\u001b[1;32m    157\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m payload:  \u001b[39m# Why are these OKAY responses?\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tsla_bot/lib/python3.8/site-packages/prawcore/auth.py:38\u001b[0m, in \u001b[0;36mBaseAuthenticator._post\u001b[0;34m(self, url, success_status, **data)\u001b[0m\n\u001b[1;32m     30\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_requestor\u001b[39m.\u001b[39mrequest(\n\u001b[1;32m     31\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     32\u001b[0m     url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     headers\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mConnection\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mclose\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m     36\u001b[0m )\n\u001b[1;32m     37\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m!=\u001b[39m success_status:\n\u001b[0;32m---> 38\u001b[0m     \u001b[39mraise\u001b[39;00m ResponseException(response)\n\u001b[1;32m     39\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "\u001b[0;31mResponseException\u001b[0m: received 401 HTTP response"
     ]
    }
   ],
   "source": [
    "# Scraping data from reddit\n",
    "select_scrap_type(\"Hot\")\n",
    "dfReddit = load_comments(1, select_scrap_type(\"Hot\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our comments, let's use the [`pandas.Series.str.slice()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.slice.html) method to ensure our data will play nicely with our model. (`bert` only supports up to a maximum of 512 tokens, so a range of `0` to `512` seems appropriate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice comments as bert supports only 512 tokens\n",
    "dfReddit['comment'] = ### YOUR LINE OF CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Analyze the Comments! üîç\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using the `FourthBrain/bert_model_reddit_tsla` model (found [here](https://huggingface.co/FourthBrain/bert_model_reddit_tsla)), which is based off of the `distilbert-base-uncased` (found [here](https://huggingface.co/distilbert-base-uncased))\n",
    "\n",
    "We'll use this model in a `sentiment-analysis` pipeline! (read all about that [here](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.pipeline.example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# initialize the sentiment pipeline\n",
    "sentiment_pipeline = ### YOUR LINE OF CODE HERE\n",
    "\n",
    "\n",
    "reddit_json = sentiment_pipeline(dfReddit[\"comment\"].tolist())\n",
    "\n",
    "# Retrieve labels and scores\n",
    "dfReddit['label'] = [reddit_json[i]['label'] for i in range(0, len(reddit_json))]\n",
    "dfReddit['score'] = [reddit_json[i]['score'] for i in range(0, len(reddit_json))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, we'll want to perform the following steps\n",
    "\n",
    "1. Convert the `created_at` column to the appropriate format and name it `'timestamp'`\n",
    "2. Normalize the dates using the `NormalizeDates()` helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from BotUtils import NormalizeDates\n",
    "\n",
    "# Rename timestamp column \n",
    "dfReddit = dfReddit.rename(columns={'created_at': 'timestamp'})\n",
    "dfReddit['timestamp'] = [datetime.fromtimestamp(dt) for dt in dfReddit['timestamp'] ]\n",
    "\n",
    "# Normalize Reddit sentiment data\n",
    "dfReddit = NormalizeDates(dfReddit, timestamp_col=\"timestamp\", interval=interval)\n",
    "\n",
    "# Create a continous time series \n",
    "dfTimeSeries = pd.DataFrame(dfReddit[\"timestamp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Get Stock Data üöÄ\n",
    "\n",
    "Next up, we're going to get the stock data and create a DataFrame - as well as normalize the dates, just like we did with the subreddit comment timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BotUtils import GetStockData\n",
    "\n",
    "# Get Stock Data\n",
    "j, dfStockData = GetStockData(settings.stock_data_api_key, symbol=symbol, start_date=start_date, end_date=end_date, interval='1day')\n",
    "dfStockData = dfStockData[(dfStockData['timestamp'] >= start_date) & (dfStockData['timestamp'] <= end_date)]\n",
    "\n",
    "# Normalize stock price data\n",
    "dfStockData = NormalizeDates(dfStockData, timestamp_col=\"timestamp\", interval=interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Merge Sentiment and Stock Data üß¨\n",
    "\n",
    "We've got some processing to do! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join time series df with stock price df and reddit df\n",
    "dfSentiment = dfTimeSeries.merge(dfStockData, how='left', on=\"timestamp\")\n",
    "dfSentiment = dfSentiment.merge(dfReddit, how='left', on=\"timestamp\")\n",
    "\n",
    "# Filter down to only the columns that we'll be using \n",
    "dfSentiment = dfSentiment[['timestamp', 'close', 'volume', 'label', 'score']]\n",
    "\n",
    "# Clean up NaNs for closing price, score, and labels\n",
    "dfSentiment['close'] = dfSentiment['close'].fillna(0)\n",
    "dfSentiment['score'] = dfSentiment['score'].fillna(0)\n",
    "dfSentiment['label'] = dfSentiment['label'].fillna('NEUTRAL')\n",
    "dfSentiment['label'] = ['NEGATIVE' for s in dfSentiment['label'] == 'LABEL_0']\n",
    "dfSentiment['label'] = ['POSITIVE' for s in dfSentiment['label'] == 'LABEL_1']\n",
    "\n",
    "# Convert close from string to float\n",
    "dfSentiment['close'] = dfSentiment['close'].astype('float')\n",
    "\n",
    "# Calculate weighted sentiment\n",
    "dfSentiment['sentiment'] = [1 if sentiment == \"POSITIVE\"  else 0 if sentiment == \"NEUTRAL\" else -1 for sentiment in dfSentiment['label'].tolist() ]\n",
    "dfSentiment['weighted_sentiment'] = dfSentiment['sentiment'] * dfSentiment['score']\n",
    "\n",
    "# Count only the POSITIVE and NEGATIVE labels (NEUTRAL is just a filler for missing dates)\n",
    "dfSentiment['counter'] = [1 if sentiment == \"POSITIVE\"  else 1 if sentiment == \"NEGATIVE\" else 0 for sentiment in dfSentiment['label'].tolist() ]\n",
    "\n",
    "# Group by to calculate Reddit post count and sentiment score (mean of weighted sentiment)\n",
    "dfSentiment = dfSentiment.groupby('timestamp') \\\n",
    "       .agg({'counter':'sum', 'close':'max', 'volume':max, 'weighted_sentiment':'mean'}) \\\n",
    "       .rename(columns={'sentiment':'count_posts', 'weighted_sentiment':'sentiment_score'}) \\\n",
    "       .reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Final Computation üñ•Ô∏è\n",
    "\n",
    "In this last step, we're going to finish up some calculations:\n",
    "\n",
    "1. Create the lags and percentage change for the closing stock price. \n",
    "2. Calculate the 3 month rolling average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lags and %change for closing stock price\n",
    "dfSentiment['close_lag1'] = dfSentiment['close'].shift(1)\n",
    "dfSentiment['close_lag1'] = dfSentiment['close_lag1'].fillna(0)\n",
    "dfSentiment['perc_change_close'] = (dfSentiment['close'] - dfSentiment['close_lag1']) / dfSentiment['close_lag1']\n",
    "dfSentiment['perc_change_close'] = dfSentiment['perc_change_close'].fillna(0)\n",
    "\n",
    "dfSentiment['sentiment_score_lag1'] = dfSentiment['sentiment_score'].shift(1)\n",
    "dfSentiment['sentiment_score_lag1'] = dfSentiment['sentiment_score_lag1'].fillna(0)\n",
    "dfSentiment['perc_change_sentiment'] = (dfSentiment['sentiment_score'] - dfSentiment['sentiment_score_lag1']) / dfSentiment['sentiment_score_lag1']\n",
    "\n",
    "# Calculate 3 month rolling average\n",
    "dfSentiment['sentiment_SMA3mo'] = dfSentiment.sentiment_score.rolling(3).mean()\n",
    "dfSentiment['sentiment_SMA3mo'] = dfSentiment['sentiment_SMA3mo'].fillna(0)\n",
    "\n",
    "dfSentiment = dfSentiment[1:]\n",
    "dfSentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save our work in a `.csv` for use later in the Streamlit app!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame locally (or somewhere else) for use in Streamlit app\n",
    "dfSentiment.to_csv('./sentiment_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8a: Plotting our Results! üìä\n",
    "\n",
    "Now, after all that, let's plot our results and see how they shape up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax1 = dfSentiment.plot(kind = 'line', x = 'timestamp',\n",
    "                  y = 'sentiment_score', color = 'Red',\n",
    "                  linewidth = 3)\n",
    "\n",
    "ax2 = dfSentiment.plot(kind = 'line', x = 'timestamp',\n",
    "                   y = 'close', secondary_y = True,\n",
    "                   color = 'Blue',  linewidth = 3,\n",
    "                   ax = ax1) \n",
    "\n",
    "#labeling x and y-axis\n",
    "ax1.set_xlabel('Timestamp', color = 'black')\n",
    "ax1.set_ylabel('Sentiment Score', color = \"r\")\n",
    "ax2.set_ylabel('% Change Stock Price', color = 'b')\n",
    " \n",
    "#defining display layout\n",
    "plt.tight_layout()\n",
    " \n",
    "#show plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('tsla_bot')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "f6bf4a95b2f6c2d430af16d3f0dcfdba9b6cfcb6cf6d1d29fbee8013a7700fcc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
